{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.statistics.traces.generic.log import case_statistics\n",
    "from pm4py.statistics.sojourn_time.log import get as soj_time_get\n",
    "import statistics as stats\n",
    "import pm4py\n",
    "import pandas as pd\n",
    "from decimal import Decimal\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from scipy.stats import norm\n",
    "from pm4py.statistics.attributes.log import get as attributes_get\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log import\n",
    "### Import of logs for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logA = xes_importer.apply(\"../logNormal.xes\")\n",
    "# logB = xes_importer.apply(\"../logFreq.xes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the variants of a log\n",
    "def get_variants(log):\n",
    "    variants = case_statistics.get_variant_statistics(log)\n",
    "    return(variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the the frequency of each event in all the cases\n",
    "def freq_attributes(log):\n",
    "    attr_list = pm4py.statistics.attributes.log.get.get_attribute_values(log, attribute_key=\"concept:name\")\n",
    "    attr_list_freq = dict.fromkeys(attr_list, 0)\n",
    "    temp = attr_list_freq\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        temp = dict.fromkeys(temp, 0)\n",
    "        i = i+1\n",
    "        for event in trace:\n",
    "            cn = event.get(\"concept:name\")\n",
    "            temp[cn] = temp[cn] +1 \n",
    "        for a, value in attr_list_freq.items():\n",
    "            if(value == 0): value = []\n",
    "            value.append(temp[a])\n",
    "            attr_list_freq[a] = value\n",
    "    return attr_list_freq\n",
    "\n",
    "def get_activity_freq_stats(log):\n",
    "    \"\"\"\n",
    "    Get frequency statisics\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    log\n",
    "        Log\n",
    "\n",
    "    Returns\n",
    "    --------------\n",
    "    map\n",
    "        \"Sum\" : fsum, \"Mean\": fmean, \"Median\": fmedian, \"StDev\": fstdev, \"Min\": fmin, \"Max\": fmax\n",
    "    \"\"\"\n",
    "    fr = freq_attributes(log)\n",
    "    fmean = {}\n",
    "    fmedian = {}\n",
    "    fmin = {}\n",
    "    fmax = {}\n",
    "    fstdev = {}\n",
    "    fsum = {}\n",
    "    # Sum\n",
    "    for key, value in fr.items():\n",
    "        fsum[key] = sum(value)\n",
    "    # Mean\n",
    "    for key, value in fr.items():\n",
    "        fmean[key] = stats.mean(value)\n",
    "    # Median\n",
    "    for key, value in fr.items():\n",
    "        fmedian[key] = stats.median(value)\n",
    "    # Min\n",
    "    for key, value in fr.items():\n",
    "        fmin[key] = min(value)\n",
    "    # Max\n",
    "    for key, value in fr.items():\n",
    "        fmax[key] = max(value)\n",
    "    for key, value in fr.items():\n",
    "        fstdev[key] = stats.stdev(value)\n",
    "\n",
    "    # print(f\"Sum: {fsum}, \\n Mean {fmean}, \\n Median {fmedian}, \\n StDev {fstdev}, \\n Min {fmin}, \\n Max {fmax}\")\n",
    "    return({\"Sum\" : fsum, \"Mean\": fmean, \"Median\": fmedian, \"StDev\": fstdev, \"Min\": fmin, \"Max\": fmax})\n",
    "    # return(fsum, fmean, fmedian, fstdev, fmin, fmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Start/Completion Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the avg start and complete time, and the median start and completion time\n",
    "def activity_time(log, attr):\n",
    "    attr_list = pm4py.statistics.attributes.log.get.get_attribute_values(log, attribute_key=\"concept:name\")\n",
    "    attr_list_time = dict.fromkeys(attr_list, 0)\n",
    "    for trace in log:\n",
    "        for event in trace:\n",
    "            temp = attr_list_time.get(event.get(\"concept:name\"))\n",
    "            if(temp == 0):\n",
    "                temp = [] \n",
    "            temp.append(event.get(attr))\n",
    "            attr_list_time[event.get(\"concept:name\")] = temp\n",
    "    return attr_list_time\n",
    "\n",
    "def get_activity_start_complete_time(log):\n",
    "    activity_start_time = activity_time(log, \"start_timestamp\")\n",
    "    activity_completion_time = activity_time(log, \"time:timestamp\")\n",
    "\n",
    "    x= {}\n",
    "    y= {}\n",
    "    j= {}\n",
    "    k= {}\n",
    "    for key in activity_start_time.keys():\n",
    "        mean_start_time = pd.to_timedelta(pd.Series(activity_start_time[key]).dt.hour, unit='H').mean()\n",
    "        x[key] = mean_start_time\n",
    "    for key in activity_completion_time.keys():\n",
    "        mean_completion_time = pd.to_timedelta(pd.Series(activity_completion_time[key]).dt.hour, unit='H').mean()\n",
    "        y[key] = mean_completion_time\n",
    "    for key in activity_start_time.keys():\n",
    "        median_start_time = pd.to_timedelta(pd.Series(activity_start_time[key]).dt.hour, unit='H').median()\n",
    "        j[key] = median_start_time\n",
    "    for key in activity_completion_time.keys():\n",
    "        median_completion_time = pd.to_timedelta(pd.Series(activity_completion_time[key]).dt.hour, unit='H').median()\n",
    "        k[key] = median_completion_time\n",
    "    return({\"Mean_start_time\": x, \"Mean_completion_time\" : y, \"Median_start_time\": j, \"Median_completion_time\" : k})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the avg duration of each activity identifier in a log\n",
    "def get_activity_duration(log):\n",
    "    soj_time = soj_time_get.apply(log, parameters={soj_time_get.Parameters.TIMESTAMP_KEY: \"time:timestamp\", soj_time_get.Parameters.START_TIMESTAMP_KEY: \"start_timestamp\"})\n",
    "    return soj_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity_duration_stats(log, minutes: bool = False):\n",
    "    dmean = soj_time_get.apply(log, parameters={soj_time_get.Parameters.TIMESTAMP_KEY: \"time:timestamp\", soj_time_get.Parameters.START_TIMESTAMP_KEY: \"start_timestamp\", soj_time_get.Parameters.AGGREGATION_MEASURE: 'mean'})\n",
    "    dmedian = soj_time_get.apply(log, parameters={soj_time_get.Parameters.TIMESTAMP_KEY: \"time:timestamp\", soj_time_get.Parameters.START_TIMESTAMP_KEY: \"start_timestamp\", soj_time_get.Parameters.AGGREGATION_MEASURE: 'median'})\n",
    "    dmin = soj_time_get.apply(log, parameters={soj_time_get.Parameters.TIMESTAMP_KEY: \"time:timestamp\", soj_time_get.Parameters.START_TIMESTAMP_KEY: \"start_timestamp\", soj_time_get.Parameters.AGGREGATION_MEASURE: 'min'})\n",
    "    dmax = soj_time_get.apply(log, parameters={soj_time_get.Parameters.TIMESTAMP_KEY: \"time:timestamp\", soj_time_get.Parameters.START_TIMESTAMP_KEY: \"start_timestamp\", soj_time_get.Parameters.AGGREGATION_MEASURE: 'max'})\n",
    "    dstdev = get_dur_stdev(log)\n",
    "    if(minutes == True):\n",
    "        for act, val in dmean.items():\n",
    "            mins = val / 60;\n",
    "            dmean[act] = round(mins)\n",
    "        for act, val in dmedian.items():\n",
    "            mins = val / 60;\n",
    "            dmedian[act] = round(mins)\n",
    "        for act, val in dmin.items():\n",
    "            mins = val / 60;\n",
    "            dmin[act] = round(mins)\n",
    "        for act, val in dmax.items():\n",
    "            mins = val / 60;\n",
    "            dmax[act] = round(mins)\n",
    "        for act, val in dstdev.items():\n",
    "            mins = val / 60;\n",
    "            dstdev[act] = round(mins)\n",
    "    \n",
    "    return({\"Mean\": dmean, \"Median\": dmedian, \"Min\": dmin, \"Max\": dmax, 'StDev': dstdev})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the stdev of the duration of each activity identifier in a log\n",
    "def get_dur_minmax(log):\n",
    "    attr_list = pm4py.statistics.attributes.log.get.get_attribute_values(log, attribute_key=\"concept:name\")\n",
    "    d = {}\n",
    "    for act in attr_list.keys():\n",
    "        d[act] = []\n",
    "    for trace in log:\n",
    "        for event in trace:\n",
    "            end = event.get(\"time:timestamp\")\n",
    "            start = event.get(\"start_timestamp\")\n",
    "            duration = end - start    \n",
    "            duration_in_s = round(duration.total_seconds())\n",
    "            d[event.get(\"concept:name\")].append(duration_in_s)\n",
    "    minr = {}\n",
    "    maxr = {}\n",
    "    for activity, values in d.items():\n",
    "        minv = min(values)\n",
    "        maxv = max(values)\n",
    "        minr[activity] = minv\n",
    "        maxr[activity] = maxv\n",
    "    return(minr, maxr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the stdev of the duration of each activity identifier in a log\n",
    "def get_dur_stdev(log):\n",
    "    attr_list = pm4py.statistics.attributes.log.get.get_attribute_values(log, attribute_key=\"concept:name\")\n",
    "    d = {}\n",
    "    for act in attr_list.keys():\n",
    "        d[act] = []\n",
    "    for trace in log:\n",
    "        for event in trace:\n",
    "            end = event.get(\"time:timestamp\")\n",
    "            start = event.get(\"start_timestamp\")\n",
    "            duration = end - start    \n",
    "            duration_in_s = round(duration.total_seconds())\n",
    "            d[event.get(\"concept:name\")].append(duration_in_s)\n",
    "    stdev = {}\n",
    "    for activity, values in d.items():\n",
    "        mm = stats.stdev(values)\n",
    "        if(mm < 1):\n",
    "            mm = 0\n",
    "        stdev[activity] = mm\n",
    "    return(stdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the median of the duration of each activity identifier in a log\n",
    "def get_dur_median(log):\n",
    "    attr_list = pm4py.statistics.attributes.log.get.get_attribute_values(log, attribute_key=\"concept:name\")\n",
    "    d = {}\n",
    "    for act in attr_list.keys():\n",
    "        d[act] = []\n",
    "    for trace in log:\n",
    "        for event in trace:\n",
    "            end = event.get(\"time:timestamp\")\n",
    "            start = event.get(\"start_timestamp\")\n",
    "            duration = end - start    \n",
    "            duration_in_s = round(duration.total_seconds())\n",
    "            d[event.get(\"concept:name\")].append(duration_in_s)\n",
    "    medians = {}\n",
    "    for activity, values in d.items():\n",
    "        mm = stats.median(values)\n",
    "        medians[activity] = mm\n",
    "    return(medians)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STATISTICAL CONFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness of frequencies based on the normal distribution of freq\n",
    "def get_freq_fitness(logNormal, logComp):\n",
    "    u = get_activity_freq_stats(logNormal)\n",
    "    attr_list = pm4py.statistics.attributes.log.get.get_attribute_values(logNormal, attribute_key=\"concept:name\")\n",
    "    attr_list_freq = dict.fromkeys(attr_list, 0)\n",
    "    temp = attr_list_freq\n",
    "    result = {}\n",
    "    i = 0\n",
    "    for trace in logComp:\n",
    "        temp = dict.fromkeys(temp, 0)\n",
    "        i = i+1\n",
    "        fitness = []\n",
    "        for event in trace:\n",
    "            cn = event.get(\"concept:name\")\n",
    "            temp[cn] = temp[cn] +1 \n",
    "        # now that I counted the occurences in this trace, I compare the values with the normal one\n",
    "        # for each temp activity, I check the probability of the actual activity frequency to fit\n",
    "        # in the normal distribution of the normal log, using the normal mean and stdev\n",
    "        for key, value in temp.items():\n",
    "            # nAvg = u.get(avgtype).get(key)\n",
    "            nAvg = u.get('Mean').get(key)\n",
    "            # nMedian = u.get(\"Median\").get(key)\n",
    "            nStdev = u.get(\"StDev\").get(key)\n",
    "            nMin = u.get(\"Min\").get(key)\n",
    "            nMax = u.get(\"Max\").get(key)\n",
    "            #  worst case in which the value is < nMin or > nMax\n",
    "            if value < nMin or value > nMax:\n",
    "                fitness.append(0)\n",
    "            # optimum case where the stdev = 0 and avg = value, meaning that there is a perfect fit\n",
    "            elif(nStdev == 0 and value == nAvg): \n",
    "                fitness.append(1)\n",
    "            else:\n",
    "                f = norm.pdf(value, loc = nAvg , scale = nStdev)\n",
    "                g = norm.pdf(nAvg, loc = nAvg , scale = nStdev)\n",
    "                fitness.append(round(f/g,5))\n",
    "        for k in temp.keys():\n",
    "            temp[k] = \"\"\n",
    "        # trace index: trace.attributes.get(\"concept:name\")\n",
    "        result[trace.attributes.get(\"concept:name\")] = fitness\n",
    "    tot = {}\n",
    "    for index, values in result.items():\n",
    "        tot[index] = np.mean(values)\n",
    "    return(result, tot, np.mean(list(tot.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#########################################\n",
    "# The assumption is that if stdev < 1, meaning that the difference is lower than 1 seconds, we round it to zero\n",
    "#########################################\n",
    "\n",
    "# Fitness of duration based on the normal distribution of the normal log\n",
    "# and comparing the probability of the new log of being in the distribution function\n",
    "def get_duration_fitness(logNormal, logComp):\n",
    "    # this is my ground truth\n",
    "    u = get_activity_duration_stats(logNormal)\n",
    "    attr_list = pm4py.statistics.attributes.log.get.get_attribute_values(logNormal, attribute_key=\"concept:name\")\n",
    "    d = {}\n",
    "    result = {}\n",
    "    for act in attr_list.keys():\n",
    "        d[act] = []\n",
    "    for trace in logComp:\n",
    "        ddict = {}\n",
    "        fitness = []\n",
    "        for event in trace:\n",
    "            end = event.get(\"time:timestamp\")\n",
    "            start = event.get(\"start_timestamp\")\n",
    "            duration = end - start    \n",
    "            duration_in_s = round(duration.total_seconds())\n",
    "            # duration_in_s = round(duration.total_seconds()/60)\n",
    "            if event.get(\"concept:name\") in ddict:\n",
    "                ddict[event.get(\"concept:name\")].append(duration_in_s)    \n",
    "            else: ddict[event.get(\"concept:name\")] = [duration_in_s]\n",
    "        for activity, values in ddict.items():\n",
    "            dstdev = u.get(\"StDev\").get(activity)\n",
    "            dmean = u.get(\"Mean\").get(activity)\n",
    "            ccmean = stats.mean(values)\n",
    "            if dstdev == 0 and dmean == ccmean: fitness.append(1)\n",
    "            elif(Decimal(min(values)) < Decimal(u.get(\"Min\").get(activity)) or Decimal(max(values)) > Decimal(u.get(\"Max\").get(activity))):\n",
    "                fitness.append(0)\n",
    "            else:\n",
    "                f = norm.pdf(ccmean, loc = dmean , scale = dstdev)\n",
    "                g = norm.pdf(dmean, loc = dmean , scale = dstdev)\n",
    "                fitness.append(round(f/g,5))\n",
    "        result[trace.attributes.get(\"concept:name\")] = fitness\n",
    "    tot = {}\n",
    "    for index, values in result.items():\n",
    "        tot[index] = np.mean(values)    \n",
    "    return(result, tot, np.mean(list(tot.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evdistribution_intersection(logNormal, logComp, distr_type=\"hours\"):\n",
    "    \"\"\"\n",
    "    Compute the intersection of the distribution of events over time\n",
    "\n",
    "    Parameters\n",
    "    ----------------\n",
    "    logNormal\n",
    "        Event log\n",
    "    logComp\n",
    "        Event log\n",
    "    distr_type\n",
    "        Type of distribution (default: days_week):\n",
    "        - days_month => Gets the distribution of the events among the days of a month (from 1 to 31)\n",
    "        - months => Gets the distribution of the events among the months (from 1 to 12)\n",
    "        - years => Gets the distribution of the events among the years of the event log\n",
    "        - hours => Gets the distribution of the events among the hours of a day (from 0 to 23)\n",
    "        - days_week => Gets the distribution of the events among the days of a week (from Monday to Sunday)\n",
    "        - weeks => Gets the distribution of the events among the weeks of a year (from 0 to 52)\n",
    "    \"\"\"\n",
    "    # pm4py.view_events_distribution_graph(logS1d, distr_type=\"hours\", format=\"png\")\n",
    "    x, y = attributes_get.get_events_distribution(logNormal, distr_type=distr_type, parameters=pm4py.utils.get_properties(logNormal))\n",
    "    x1, y1 = attributes_get.get_events_distribution(logComp, distr_type=distr_type, parameters=pm4py.utils.get_properties(logComp))\n",
    "\n",
    "    if(max(y) > max(y1)) : max_hist = max(y) \n",
    "    else: max_hist = max(y1)\n",
    "    if(min(y) > min(y1)) : min_hist = min(y) \n",
    "    else: min_hist = min(y1)\n",
    "\n",
    "    hist_1, _ = np.histogram(y, range=[min_hist,max_hist])\n",
    "    hist_2, _ = np.histogram(y1, range=[min_hist,max_hist])\n",
    "\n",
    "    def return_intersection(hist_1, hist_2):\n",
    "        minima = np.minimum(hist_1, hist_2)\n",
    "        intersection = np.true_divide(np.sum(minima), np.sum(hist_2))\n",
    "        return intersection\n",
    "\n",
    "    intersection = return_intersection(hist_1, hist_2)\n",
    "\n",
    "    if distr_type == \"days_month\":\n",
    "        title = \"Distribution of the Events over the Days of a Month\";\n",
    "        x_axis = \"Day of month\";\n",
    "    elif distr_type == \"months\":\n",
    "        title = \"Distribution of the Events over the Months\";\n",
    "        x_axis = \"Month\";\n",
    "    elif distr_type == \"years\":\n",
    "        title = \"Distribution of the Events over the Years\";\n",
    "        x_axis = \"Year\";\n",
    "    elif distr_type == \"hours\":\n",
    "        title = \"Distribution of the Events over the Hours\";\n",
    "        x_axis = \"Hour (of day)\";\n",
    "    elif distr_type == \"days_week\":\n",
    "        title = \"Distribution of the Events over the Days of a Week\";\n",
    "        x_axis = \"Day of the Week\";\n",
    "    elif distr_type == \"weeks\":\n",
    "        title = \"Distribution of the Events over the Weeks of a Year\";\n",
    "        x_axis = \"Week of the Year\";\n",
    "\n",
    "    plt.plot(y, 'bo', alpha=0.5)\n",
    "    plt.plot(y1, 'ro', alpha=0.5)\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel('Number of events')\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(x_axis)\n",
    "    plt.ylabel('Number of events')\n",
    "    plt.title(title)\n",
    "    plt.bar(x,y, color='b', alpha=0.5)\n",
    "    plt.bar(x1,y1, color='r',  alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    return intersection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_evdistribution_intersection(logS1n, logTest3)\n",
    "# x, y = attributes_get.get_events_distribution(logS1n, distr_type=\"hours\", parameters=pm4py.utils.get_properties(logS1n))\n",
    "# print(y)\n",
    "\n",
    "def get_freq_hour_normalized2(log):\n",
    "    \"\"\"\n",
    "    Return the frequency of each activity, per hour, normalized\n",
    "\n",
    "    Parameters\n",
    "    ----------------\n",
    "    log\n",
    "        Event log\n",
    "    \"\"\"\n",
    "    attr_list = pm4py.statistics.attributes.log.get.get_attribute_values(log, attribute_key=\"concept:name\")\n",
    "    res = {key: {} for key in attr_list}\n",
    "\n",
    "    for trace in log:\n",
    "        for event in trace:\n",
    "            if len(res[event.get(\"concept:name\")]) == 0:\n",
    "                # The dict is empty\n",
    "                res[event.get(\"concept:name\")][event.get(\"start_timestamp\").hour] = 1\n",
    "            elif event.get(\"start_timestamp\").hour not in res[event.get(\"concept:name\")]:\n",
    "                res[event.get(\"concept:name\")][event.get(\"start_timestamp\").hour] = 1\n",
    "            else:\n",
    "                res[event.get(\"concept:name\")][event.get(\"start_timestamp\").hour] += 1\n",
    "\n",
    "    for activity, values in res.items():\n",
    "        # The normalization is between zero and the max value that each activity can have\n",
    "        maxres = max(values.values())\n",
    "        # minres = min(values.values())\n",
    "        minres = 0\n",
    "        for hour, value in values.items():\n",
    "            res[activity][hour] = (value - minres)/(maxres - minres)\n",
    "    return res\n",
    "\n",
    "def get_time_fitness2(logA, logB):\n",
    "    nlog = get_freq_hour_normalized2(logA)\n",
    "    attr_list = nlog.keys()\n",
    "    trace_fitness = {} \n",
    "    for trace in logB:\n",
    "        x = {key: {} for key in attr_list}\n",
    "        fitness = []\n",
    "        for event in trace:\n",
    "            if len(x[event.get(\"concept:name\")]) == 0:\n",
    "                # The dict is empty\n",
    "                x[event.get(\"concept:name\")][event.get(\"start_timestamp\").hour] = 1\n",
    "            elif event.get(\"start_timestamp\").hour not in x[event.get(\"concept:name\")]:\n",
    "                x[event.get(\"concept:name\")][event.get(\"start_timestamp\").hour] = 1\n",
    "            else:\n",
    "                x[event.get(\"concept:name\")][event.get(\"start_timestamp\").hour] += 1\n",
    "        for act, values in x.items():\n",
    "            for hour,freq in values.items():\n",
    "                # we have to normalize each value:\n",
    "                freq_norm = (freq - 0)/(max(values.values()) - 0)\n",
    "                if hour not in nlog[act]:\n",
    "                    fitness.append(0)\n",
    "                # The frequency in the normal log is zero, but the activity is executed in the log to compare\n",
    "                elif freq_norm > 0 and nlog[act][hour] == 0:\n",
    "                    fitness.append(0)\n",
    "                elif freq_norm >= nlog[act][hour]:\n",
    "                    fitness.append(1)\n",
    "                elif freq_norm < nlog[act][hour]:\n",
    "                    fitness.append( freq_norm/nlog[act][hour])\n",
    "        trace_fitness[trace.attributes.get(\"concept:name\")] = fitness\n",
    "    result = {}\n",
    "    for id, trace in trace_fitness.items():\n",
    "        result[id] = np.mean(trace)\n",
    "    return(trace_fitness, result, np.mean(list(result.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_hour_normalized(log):\n",
    "    \"\"\"\n",
    "    Return the frequency of each activity, per hour, normalized\n",
    "\n",
    "    Parameters\n",
    "    ----------------\n",
    "    log\n",
    "        Event log\n",
    "    \"\"\"\n",
    "    # temp = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "    # res = {key: 0 for key in temp}\n",
    "    attr_list = pm4py.statistics.attributes.log.get.get_attribute_values(log, attribute_key=\"concept:name\")\n",
    "\n",
    "    res = {}\n",
    "    for a in attr_list:\n",
    "        res[a] = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0}\n",
    "\n",
    "    for trace in log:\n",
    "        for event in trace:\n",
    "            res[event.get(\"concept:name\")][event.get(\"start_timestamp\").hour] += 1\n",
    "\n",
    "    for activity, values in res.items():\n",
    "        maxres = max(values.values())\n",
    "        minres = min(values.values())\n",
    "        for hour, value in values.items():\n",
    "            res[activity][hour] = (value - minres)/(maxres - minres)\n",
    "    return res\n",
    "\n",
    "def return_intersection(hist_1, hist_2):\n",
    "    minima = np.minimum(hist_1, hist_2)\n",
    "    intersection = np.true_divide(np.sum(minima), np.sum(hist_2))\n",
    "    return intersection\n",
    "\n",
    "def get_time_fitness(logA, logB):\n",
    "    nlog = get_freq_hour_normalized(logA)\n",
    "    nlog2 = get_freq_hour_normalized(logB)\n",
    "    arr = []\n",
    "    inters = {}\n",
    "\n",
    "    for activity, normalized in nlog.items():\n",
    "        # plt.bar(normalized.keys(),normalized.values(), alpha= 0.5)\n",
    "        # plt.bar(nlog2[activity].keys(), nlog2[activity].values(), alpha= 0.5)\n",
    "        # plt.title(activity)\n",
    "        # plt.show()\n",
    "\n",
    "        for index, value in normalized.items():\n",
    "            if(value == 0 and nlog2[activity][index] == 0):\n",
    "                pass\n",
    "            elif value == 0: arr.append(0.0)\n",
    "            elif nlog2[activity][index] > value:  arr.append(1)\n",
    "            else:\n",
    "                arr.append(nlog2[activity][index]/value)\n",
    "\n",
    "        # hist_1, _ = np.histogram(list(nlog[activity].values()), range=[0,1])\n",
    "        # hist_2, _ = np.histogram(list(nlog2[activity].values()), range=[0,1])\n",
    "        # inters[activity] = return_intersection(hist_1, hist_2)\n",
    "    return(arr, sum(arr)/len(arr))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1b5acb681577528e2aff85da35659397b1a3d39ac070b6fb3442068ab6131df9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
